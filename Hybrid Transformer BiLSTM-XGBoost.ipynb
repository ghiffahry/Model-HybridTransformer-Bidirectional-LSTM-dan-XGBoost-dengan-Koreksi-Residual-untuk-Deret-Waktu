{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9251211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# hybrid_transformer_BiLSTM_XGBoost_full_pipeline.py\n",
    "# Penulis: Ghiffary\n",
    "# Tujuan: pipeline peramalan dengan koreksi galat pemodelan\n",
    "# Data: Business-day 2014-01-24 to 2025-06-24\n",
    "# 1 endogenous (Price) + 17 exogenous dummies + engineered features\n",
    "# =====================\n",
    "\n",
    "import os, random\n",
    "import numpy as np, pandas as pd\n",
    "import optuna, shap, tensorflow as tf\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras import Model, Input, regularizers\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Bidirectional, Dense, Dropout,\n",
    "    LayerNormalization, MultiHeadAttention, Add\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 0. Reproducibility & Settings\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "SEED = 86\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "sns.set(style=\"whitegrid\", rc={\"figure.figsize\": (12,6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ccbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Plot ACF & PACF \n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def plot_acf_pacf(df, lags=60):\n",
    "    price = df['Price'].values\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14,4))\n",
    "    sm.graphics.tsa.plot_acf(price, lags=lags, ax=axes[0])\n",
    "    axes[0].set_title('ACF of Price')\n",
    "    sm.graphics.tsa.plot_pacf(price, lags=lags, ax=axes[1])\n",
    "    axes[1].set_title('PACF of Price')\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499efb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Pemodelan Fitur Temporal\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def load_and_fe(path, lags=[7,22], mas=[7,22], fourier=[5,21]):\n",
    "    df = pd.read_csv(path, parse_dates=['date'], index_col='date').sort_index()\n",
    "    # lag features\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df['Price'].shift(lag)\n",
    "    # moving averages\n",
    "    for w in mas:\n",
    "        df[f'ma_{w}'] = df['Price'].rolling(w).mean()\n",
    "    # fourier terms\n",
    "    t = np.arange(len(df))\n",
    "    for p in fourier:\n",
    "        df[f'sin_{p}'] = np.sin(2*np.pi*t/p)\n",
    "        df[f'cos_{p}'] = np.cos(2*np.pi*t/p)\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Split Data 80:20\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def train_test_split_time(df, test_size=0.2):\n",
    "    idx = int(len(df)*(1-test_size))\n",
    "    return df.iloc[:idx], df.iloc[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Preprocessor + Windowing + Inverse Scaling = Transformer\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "class TimeSeriesPreprocessor:\n",
    "    def __init__(self, time_steps=1):\n",
    "        self.ts = time_steps\n",
    "        self.scaler_y = MinMaxScaler()\n",
    "        self.scaler_x = StandardScaler()\n",
    "    def fit_transform(self, df):\n",
    "        y = df[['Price']].values; X = df.drop(columns=['Price']).values\n",
    "        y_s = self.scaler_y.fit_transform(y)\n",
    "        X_s = self.scaler_x.fit_transform(X)\n",
    "        return y_s, X_s\n",
    "    def transform(self, df):\n",
    "        y = df[['Price']].values; X = df.drop(columns=['Price']).values\n",
    "        return self.scaler_y.transform(y), self.scaler_x.transform(X)\n",
    "    def create_sequences(self, y_s, X_s):\n",
    "        Ys, Xp, Xe = [], [], []\n",
    "        for i in range(self.ts, len(y_s)):\n",
    "            Ys.append(y_s[i,0])\n",
    "            Xp.append(y_s[i-self.ts:i,0])\n",
    "            Xe.append(X_s[i-self.ts:i,:])\n",
    "        return np.expand_dims(np.array(Xp),-1), np.array(Xe), np.array(Ys)\n",
    "    def inv_y(self, y_s):\n",
    "        return self.scaler_y.inverse_transform(y_s.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a50800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 5. Positional Encoding Layer = Transformer\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, length, depth):\n",
    "        super().__init__()\n",
    "        pos = np.arange(length)[:,None]\n",
    "        i   = np.arange(depth)[None,:]\n",
    "        angle = pos/np.power(10000,(2*(i//2))/depth)\n",
    "        angle[:,0::2] = np.sin(angle[:,0::2])\n",
    "        angle[:,1::2] = np.cos(angle[:,1::2])\n",
    "        self.pe = tf.constant(angle[None,...],dtype=tf.float32)\n",
    "    def call(self, x):\n",
    "        return x + self.pe[:,:tf.shape(x)[1],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc553b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 6. Transformer-BiLSTM Base Model\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def build_transformer_bilstm(ts, feat_dim, p, n_blocks=3):\n",
    "    inp_y = Input((ts,1)); inp_x = Input((ts,feat_dim))\n",
    "    x = tf.concat([inp_y, inp_x],axis=-1)\n",
    "    x = PositionalEncoding(ts, x.shape[-1])(x)\n",
    "    for _ in range(n_blocks):\n",
    "        x = Bidirectional(LSTM(p['lstm_units'], return_sequences=True,\n",
    "                               dropout=p['dropout'],\n",
    "                               kernel_regularizer=regularizers.l2(p['wd'])))(x)\n",
    "        x = LayerNormalization()(x)\n",
    "        att = MultiHeadAttention(num_heads=p['n_heads'], key_dim=p['key_dim'],\n",
    "                                 dropout=p['attn_dropout'])(x,x)\n",
    "        x = Add()([x, att]); x = LayerNormalization()(x)\n",
    "        h = Dense(p['dense_units'], activation=p['activation'],\n",
    "                  kernel_regularizer=regularizers.l2(p['wd']))(x)\n",
    "        proj = Dense(x.shape[-1])(h)\n",
    "        proj = Dropout(p['dense_dropout'])(proj)\n",
    "        x = Add()([x, proj]); x = LayerNormalization()(x)\n",
    "    x = tf.reduce_mean(x, axis=1)\n",
    "    out = Dense(1)(x)\n",
    "    m = Model([inp_y, inp_x], out)\n",
    "    m.compile(Adam(p['lr']), loss='huber_loss', metrics=['mape','mae'])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 7. Hyperparameter Tuning Base Model (Optuna + Pruner)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def tune_base(Xp, Xe, y, trials=25):\n",
    "    def obj(trial):\n",
    "        p = {\n",
    "            'lstm_units': trial.suggest_int('lstm_units',64,256),\n",
    "            'dropout':    trial.suggest_float('dropout',0.1,0.6),\n",
    "            'wd':         trial.suggest_float('wd',1e-6,1e-3,log=True),\n",
    "            'n_heads':    trial.suggest_int('n_heads',2,8),\n",
    "            'key_dim':    trial.suggest_int('key_dim',4,32),\n",
    "            'attn_dropout':trial.suggest_float('attn_dropout',0.0,0.6),\n",
    "            'dense_units':trial.suggest_int('dense_units',32,128),\n",
    "            'dense_dropout':trial.suggest_float('dense_dropout',0.1,0.6),\n",
    "            'activation': trial.suggest_categorical('activation',['relu','selu']),\n",
    "            'lr':         trial.suggest_float('lr',1e-5,1e-2,log=True)\n",
    "        }\n",
    "        tr,vl = train_test_split(np.arange(len(y)), test_size=0.2, shuffle=False)\n",
    "        m = build_transformer_bilstm(Xp.shape[1], Xe.shape[2], p)\n",
    "        m.fit([Xp[tr],Xe[tr]], y[tr],\n",
    "              validation_data=([Xp[vl],Xe[vl]], y[vl]),\n",
    "              epochs=100, batch_size=64,\n",
    "              callbacks=[\n",
    "                  EarlyStopping(monitor='val_mape',patience=10,restore_best_weights=True),\n",
    "                  ReduceLROnPlateau(monitor='val_mape',patience=5,factor=0.5)\n",
    "              ], verbose=0)\n",
    "        return float(m.history.history['val_mape'][-1])\n",
    "    study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())\n",
    "    study.optimize(obj, n_trials=trials)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1883d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 8. Train Base Model on Full Train Set (dengan validasi internal)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def train_base(train_df, pre, best_params, n_blocks=3, val_split=0.1):\n",
    "    # Preprocess\n",
    "    y_s, X_s = pre.fit_transform(train_df)\n",
    "    Xp, Xe, y_seq = pre.create_sequences(y_s, X_s)\n",
    "\n",
    "    # Internal split\n",
    "    split = int(len(y_seq)*(1-val_split))\n",
    "    Xp_tr, Xp_val = Xp[:split], Xp[split:]\n",
    "    Xe_tr, Xe_val = Xe[:split], Xe[split:]\n",
    "    y_tr,  y_val  = y_seq[:split], y_seq[split:]\n",
    "\n",
    "    model = build_transformer_bilstm(Xp.shape[1], Xe.shape[2], best_params, n_blocks)\n",
    "    model.fit(\n",
    "        [Xp_tr, Xe_tr], y_tr,\n",
    "        validation_data=([Xp_val, Xe_val], y_val),   # <- sekarang tersedia val_*\n",
    "        epochs=200, batch_size=64,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='val_mape', patience=20, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_mape', patience=10, factor=0.5)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 9. Residual Feature Engineering \n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def get_residuals_and_feats(model, df, pre):\n",
    "    # 1) Transform & sequences\n",
    "    y_s, X_s = pre.transform(df)\n",
    "    Xp, Xe, y_seq = pre.create_sequences(y_s, X_s)\n",
    "    # 2) Base prediction & residual\n",
    "    y_base_s = model.predict([Xp, Xe]).flatten()\n",
    "    res_s    = y_seq - y_base_s\n",
    "\n",
    "    # 3) Build rolling features\n",
    "    df_r = pd.DataFrame({'res':res_s, 'pred':y_base_s})\n",
    "    df_r['rm']   = df_r['res'].rolling(5).mean().fillna(0)\n",
    "    df_r['rs']   = df_r['res'].rolling(5).std().fillna(0)\n",
    "    df_r['skew'] = df_r['res'].rolling(5).apply(lambda x: skew(x), raw=True).fillna(0)\n",
    "    df_r['kurt'] = df_r['res'].rolling(5).apply(lambda x: kurtosis(x), raw=True).fillna(0)\n",
    "\n",
    "    last_y = Xp[:, -1, 0].reshape(-1,1)\n",
    "    last_x = Xe[:, -1, :]\n",
    "    X_res  = np.hstack([last_y, last_x, df_r[['pred','rm','rs','skew','kurt']].values])\n",
    "\n",
    "    return y_seq, y_base_s, res_s, X_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c9b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 10. Train XGBoost on Residuals + Plot Feature Importance\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 10a. Train residual model dengan hyperparameter tuning yang lebih luas\n",
    "def train_residual_xgb(X, y, trials=50):\n",
    "    \"\"\"\n",
    "    Train XGBRegressor untuk memodelkan residual dengan RandomizedSearchCV.\n",
    "\n",
    "    Parameters:\n",
    "        X: np.array, residual features\n",
    "        y: np.array, target residual (scaled)\n",
    "        trials: int, jumlah iterasi pencarian hyperparameter\n",
    "\n",
    "    Returns:\n",
    "        best_model: trained XGBRegressor\n",
    "    \"\"\"\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'learning_rate': [0.005, 0.01, 0.05, 0.1],\n",
    "        'subsample': [0.6, 0.7, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'gamma': [0, 1, 5],\n",
    "        'reg_alpha': [0, 0.01, 0.1],\n",
    "        'reg_lambda': [0.1, 1.0, 5.0]\n",
    "    }\n",
    "\n",
    "    xgb = XGBRegressor(objective='reg:squarederror', n_jobs=-1, verbosity=0)\n",
    "    search = RandomizedSearchCV(\n",
    "        xgb,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=trials,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "\n",
    "    print(\"✅ Best XGBoost Parameters:\")\n",
    "    for k, v in search.best_params_.items():\n",
    "        print(f\"  - {k}: {v}\")\n",
    "    return search.best_estimator_\n",
    "\n",
    "# 10b. Advanced plot of XGBoost feature importances\n",
    "def plot_xgb_importance(model, feature_names, top_n=20):\n",
    "    \"\"\"\n",
    "    Plot top N feature importances from trained XGBoost model using SHAP-style barplot.\n",
    "\n",
    "    Parameters:\n",
    "        model: trained XGBRegressor\n",
    "        feature_names: list of str, feature names\n",
    "        top_n: int, number of top features to show\n",
    "    \"\"\"\n",
    "    booster = model.get_booster()\n",
    "    fmap = booster.get_score(importance_type='gain')\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    imp_df = pd.DataFrame.from_dict(fmap, orient='index', columns=['gain'])\n",
    "    imp_df.index.name = 'feature'\n",
    "    imp_df = imp_df.reset_index()\n",
    "    imp_df['index'] = imp_df['feature'].str.extract(r'f(\\d+)').astype(int)\n",
    "    imp_df['label'] = imp_df['index'].apply(lambda i: feature_names[i] if i < len(feature_names) else f'f{i}')\n",
    "    imp_df = imp_df.sort_values(by='gain', ascending=False).head(top_n)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=imp_df, x='gain', y='label', palette='rocket')\n",
    "    plt.title(f\"Top {top_n} Most Important Features (by Gain)\", fontsize=14, weight='bold')\n",
    "    plt.xlabel(\"Average Gain\", fontsize=12)\n",
    "    plt.ylabel(\"Features\", fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000abc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 11. Evaluate & Plot (Train/Test) — inverse scale & MAPE \n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def evaluate_and_plot(base_m, res_m, train_df, test_df, pre):\n",
    "    # TRAIN\n",
    "    y_tr_s, yb_tr_s, res_tr_s, Xr_tr = get_residuals_and_feats(base_m, train_df, pre)\n",
    "    yres_tr_s = res_m.predict(Xr_tr)\n",
    "    yh_tr_s   = yb_tr_s + yres_tr_s\n",
    "\n",
    "    # TEST\n",
    "    y_te_s, yb_te_s, res_te_s, Xr_te = get_residuals_and_feats(base_m, test_df, pre)\n",
    "    yres_te_s = res_m.predict(Xr_te)\n",
    "    yh_te_s   = yb_te_s + yres_te_s\n",
    "\n",
    "    # INVERSE to original scale\n",
    "    y_tr,  yb_tr,  yres_tr,  yh_tr  = pre.inv_y(y_tr_s),  pre.inv_y(yb_tr_s), pre.inv_y(yres_tr_s), pre.inv_y(yh_tr_s)\n",
    "    y_te,  yb_te,  yres_te,  yh_te  = pre.inv_y(y_te_s),  pre.inv_y(yb_te_s), pre.inv_y(yres_te_s), pre.inv_y(yh_te_s)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    def calc_metrics(a, p):\n",
    "        return {\n",
    "            'MAPE': np.mean(np.abs((a-p)/a))*100,\n",
    "            'RMSE': np.sqrt(mean_squared_error(a,p)),\n",
    "            'MAE':  mean_absolute_error(a,p),\n",
    "            'R2':   r2_score(a,p)\n",
    "        }\n",
    "\n",
    "    m_tr = calc_metrics(y_tr, yh_tr)\n",
    "    m_te = calc_metrics(y_te, yh_te)\n",
    "    print(\"Train metrics:\", m_tr)\n",
    "    print(\"Test  metrics:\", m_te)\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(y_te,        label='Actual', color='black', lw=1)\n",
    "    plt.plot(yb_te, '--',  label='Base', alpha=0.7)\n",
    "    plt.plot(yres_te, ':', label='XGB Residual', alpha=0.7)\n",
    "    plt.plot(yh_te, '-.',  label='Hybrid', color='C3', lw=2)\n",
    "    plt.title(f\"Test Set Comparison\\nMAPE={m_te['MAPE']:.2f}%, RMSE={m_te['RMSE']:.2f}, MAE={m_te['MAE']:.2f}\")\n",
    "    plt.xlabel('Time Index'); plt.ylabel('Price')\n",
    "    plt.legend(); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "    combined_actual = np.concatenate([y_tr, y_te])\n",
    "    combined_hybrid = np.concatenate([yh_tr, yh_te])\n",
    "    sep = len(y_tr)\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(combined_actual, label='Actual', color='black', lw=1)\n",
    "    plt.plot(combined_hybrid, '--', label='Hybrid', color='C3', lw=1.5)\n",
    "    plt.axvline(sep, color='red', linestyle=':', lw=2)\n",
    "    plt.text(sep+1, combined_actual.mean(), 'Test Start', color='red')\n",
    "    plt.title(\"Combined Train & Test Hybrid Forecast\")\n",
    "    plt.xlabel('Time Index'); plt.ylabel('Price')\n",
    "    plt.legend(); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1569ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 12. Forecast \n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def forecast_full_and_save(base_m, res_m, df_full, pre, prefix='hybrid'):\n",
    "    # 1) Prepare sequences & predictions\n",
    "    y_s, X_s = pre.transform(df_full)\n",
    "    Xp, Xe, y_seq = pre.create_sequences(y_s, X_s)\n",
    "    yb_s  = base_m.predict([Xp, Xe]).flatten()\n",
    "    _, _, _, Xr_full = get_residuals_and_feats(base_m, df_full, pre)\n",
    "    yres_s = res_m.predict(Xr_full)\n",
    "    yh_s   = yb_s + yres_s\n",
    "\n",
    "    # 2) Inverse scale\n",
    "    y_o  = pre.inv_y(y_seq)\n",
    "    yh_o = pre.inv_y(yh_s)\n",
    "\n",
    "    # 3) 95% CI using rolling std of training residuals\n",
    "    resid = y_o - yh_o\n",
    "    std_rolling = pd.Series(resid).rolling(window=30).std().bfill().values\n",
    "    lower = yh_o - 1.96 * std_rolling\n",
    "    upper = yh_o + 1.96 * std_rolling\n",
    "\n",
    "    # 4) Plot with CI\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(y_o,      label='Actual', color='black', lw=1)\n",
    "    plt.plot(yh_o, '--',label='Hybrid Forecast', color='C3', lw=1.5)\n",
    "    plt.fill_between(range(len(yh_o)), lower, upper,\n",
    "                     color='C3', alpha=0.2, label='95% CI')\n",
    "    plt.title('Full Data Hybrid Forecast with 95% CI')\n",
    "    plt.xlabel('Time Index'); plt.ylabel('Price')\n",
    "    plt.legend(); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # 5) Save\n",
    "    #base_m.save(f'{prefix}_base.h5')\n",
    "    #import joblib\n",
    "    #joblib.dump(res_m, f'{prefix}_xgb.pkl')\n",
    "    #joblib.dump(pre, f'{prefix}_pre.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933218fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # 1. Load data & ACF PACF\n",
    "    df_full = load_and_fe('final_data_with_dummy_anomaly.csv')\n",
    "    plot_acf_pacf(df_full, lags=60)\n",
    "\n",
    "    # 2. time_steps\n",
    "    TIME_STEPS = 22\n",
    "    logging.info(f\"Using TIME_STEPS = {TIME_STEPS}\")\n",
    "\n",
    "    # 3. Split Train/Test\n",
    "    train_df, test_df = train_test_split_time(df_full, test_size=0.2)\n",
    "    logging.info(f\"Train size = {len(train_df)}, Test size = {len(test_df)}\")\n",
    "\n",
    "    # 4. Preprocessing\n",
    "    preproc = TimeSeriesPreprocessor(time_steps=TIME_STEPS)\n",
    "    y_s, X_s = preproc.fit_transform(train_df)\n",
    "    Xp, Xe, y = preproc.create_sequences(y_s, X_s)\n",
    "\n",
    "    # 5. Tuning & train base model (Transformer-BiLSTM)\n",
    "    best_params = tune_base(Xp, Xe, y, trials=5)\n",
    "    base_model = train_base(train_df, preproc, best_params, n_blocks=3)\n",
    "\n",
    "    # 6. Residual dan Fitur\n",
    "    y_tr_s, yb_tr_s, res_tr_s, Xr_tr = get_residuals_and_feats(base_model, train_df, preproc)\n",
    "\n",
    "    # 7. Train XGBoost pada Residual\n",
    "    xgb_model = train_residual_xgb(Xr_tr, res_tr_s, trials=25)\n",
    "\n",
    "    # 8. Plot XGB feature importance\n",
    "    feat_names = ['last_price'] + list(train_df.drop(columns=['Price']).columns) + ['pred', 'rm', 'rs', 'skew', 'kurt']\n",
    "    plot_xgb_importance(xgb_model, feat_names, top_n=15)\n",
    "\n",
    "    # SHAP summary plot\n",
    "    import shap\n",
    "    explainer = shap.Explainer(xgb_model)\n",
    "    shap_values = explainer(Xr_tr[:500])\n",
    "    shap.summary_plot(shap_values, Xr_tr[:500], feature_names=feat_names, max_display=15)\n",
    "\n",
    "    # 9. Evaluasi Model\n",
    "    evaluate_and_plot(base_model, xgb_model, train_df, test_df, preproc)\n",
    "\n",
    "    # 10. Forecast\n",
    "    forecast_full_and_save(base_model, xgb_model, df_full, preproc, prefix='hybrid_model')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
